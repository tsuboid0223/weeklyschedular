import streamlit as st
import requests
import google.generativeai as genai
import time
import re
import json
import pandas as pd
from io import StringIO
from datetime import datetime
from playwright.sync_api import sync_playwright
import urllib.parse
from urllib.parse import quote_plus

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆBrowser APIç‰ˆï¼‰",
    page_icon="ğŸ§ª",
    layout="wide"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .api-status {
        padding: 0.5rem 1rem;
        border-radius: 0.3rem;
        margin: 0.5rem 0;
        font-weight: bold;
    }
    .api-success {
        background-color: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
    }
</style>
""", unsafe_allow_html=True)

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚¯ãƒ©ã‚¹
class RealTimeLogger:
    def __init__(self, container):
        self.container = container
        self.logs = []
        
    def log(self, message, level="INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"
        self.logs.append(log_entry)
        
        with self.container:
            st.code("\n".join(self.logs[-50:]), language="log")

# Gemini APIè¨­å®š
def setup_gemini():
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        # gemini-2.5-proã«å¤‰æ›´ï¼ˆæœ€æ–°ãƒ¢ãƒ‡ãƒ«ï¼‰
        return genai.GenerativeModel('gemini-2.5-pro')
    except Exception as e:
        st.error(f"âŒ Gemini APIè¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# SERP APIè¨­å®šï¼ˆGoogleæ¤œç´¢ç”¨ï¼‰
def check_serp_api_config():
    try:
        if "BRIGHTDATA_API_KEY" in st.secrets:
            return {
                'api_key': st.secrets["BRIGHTDATA_API_KEY"],
                'zone_name': st.secrets.get("BRIGHTDATA_ZONE_NAME", "serp_api1"),
                'available': True
            }
    except:
        pass
    return {'available': False}

# Browser APIè¨­å®šï¼ˆãƒšãƒ¼ã‚¸å–å¾—ç”¨ï¼‰
BROWSER_API_CONFIG = {
    'ws_endpoint': 'wss://brd-customer-hl_3c49a4bb-zone-scraping_browser1:lokq2uz6vn5q@brd.superproxy.io:9222',
    'available': True
}

# å¯¾è±¡ECã‚µã‚¤ãƒˆã®å®šç¾©ï¼ˆ11ã‚µã‚¤ãƒˆï¼‰
TARGET_SITES = {
    "cosmobio": {"name": "ã‚³ã‚¹ãƒ¢ãƒã‚¤ã‚ª", "domain": "cosmobio.co.jp"},
    "funakoshi": {"name": "ãƒ•ãƒŠã‚³ã‚·", "domain": "funakoshi.co.jp"},
    "axel": {"name": "AXEL", "domain": "axel.as-1.co.jp"},
    "selleck": {"name": "Selleck", "domain": "selleck.co.jp"},
    "mce": {"name": "MCE", "domain": "medchemexpress.com"},
    "nakarai": {"name": "ãƒŠã‚«ãƒ©ã‚¤", "domain": "nacalai.co.jp"},
    "fujifilm": {"name": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ å’Œå…‰", "domain": "labchem-wako.fujifilm.com"},
    "kanto": {"name": "é–¢æ±åŒ–å­¦", "domain": "kanto.co.jp"},
    "tci": {"name": "TCI", "domain": "tcichemicals.com"},
    "merck": {"name": "Merck", "domain": "merck.com"},
    "wako": {"name": "å’Œå…‰ç´”è–¬", "domain": "hpc-j.co.jp"}
}

def search_google_with_serp(query, serp_config, logger):
    """SERP APIçµŒç”±ã§Googleæ¤œç´¢ã‚’å®Ÿè¡Œ"""
    try:
        logger.log(f"  ğŸ” SERP APIçµŒç”±ã§Googleæ¤œç´¢: {query[:60]}...", "DEBUG")
        
        api_url = "https://api.brightdata.com/request"
        search_url = f"https://www.google.com/search?q={quote_plus(query)}&num=10&hl=ja&gl=jp"
        
        headers = {
            'Authorization': f'Bearer {serp_config["api_key"]}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'zone': serp_config['zone_name'],
            'url': search_url,
            'format': 'raw'
        }
        
        response = requests.post(api_url, headers=headers, json=payload, timeout=30)
        
        if response.status_code == 200:
            logger.log(f"  âœ… Googleæ¤œç´¢æˆåŠŸ (HTML: {len(response.text)} chars)", "DEBUG")
            return response.text
        else:
            logger.log(f"  âš ï¸ SERP API HTTP {response.status_code}", "WARNING")
            return None
            
    except Exception as e:
        logger.log(f"  âŒ SERP APIæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return None

def extract_urls_from_html(html_content, domain, logger):
    """HTMLã‹ã‚‰URLã‚’æŠ½å‡º"""
    urls = []
    
    try:
        patterns = [
            rf'href=["\']?(https?://(?:www\.)?{re.escape(domain)}[^"\'\s>]*)["\']?',
            rf'(https?://(?:www\.)?{re.escape(domain)}[^\s<>"\'()]*)',
        ]
        
        all_urls = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            
            for match in matches:
                url = match[0] if isinstance(match, tuple) else match
                
                # URLã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                # Googleãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                if '&ved=' in url:
                    url = url.split('&ved=')[0]
                elif '?ved=' in url:
                    url = url.split('?ved=')[0]
                
                # ãã®ä»–ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                for param in ['&hl=', '?hl=', '&sl=', '&tl=', '&client=']:
                    if param in url:
                        url = url.split(param)[0]
                
                # æœ«å°¾ã®è¨˜å·å‰Šé™¤
                url = url.rstrip('.,;:)"\'')
                
                # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                if url.startswith('http') and len(url) > 20:
                    exclude_patterns = ['google.com', 'youtube.com', 'translate.google', 'webcache']
                    if not any(ex in url.lower() for ex in exclude_patterns):
                        all_urls.add(url)
        
        logger.log(f"    åˆè¨ˆ {len(all_urls)} ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯URLç™ºè¦‹", "DEBUG")
        
        # URLå“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scored_urls = []
        for url in all_urls:
            score = 0
            url_lower = url.lower()
            
            if any(kw in url_lower for kw in ['product', 'item', 'detail', 'catalog', 'contents']):
                score += 10
            if re.search(r'\d{3,}', url):
                score += 5
            
            scored_urls.append((url, score))
        
        scored_urls.sort(key=lambda x: x[1], reverse=True)
        
        for url, score in scored_urls[:10]:
            urls.append({
                'url': url,
                'score': score
            })
            logger.log(f"    âœ“ URL (ã‚¹ã‚³ã‚¢:{score}): {url[:80]}...", "DEBUG")
        
        if urls:
            logger.log(f"  âœ… {len(urls)}ä»¶ã®URLæŠ½å‡ºæˆåŠŸ", "INFO")
        else:
            logger.log(f"  âš ï¸ è©²å½“URLãªã—", "WARNING")
        
        return urls
        
    except Exception as e:
        logger.log(f"  âŒ URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        return []

def clean_url(url):
    """
    URLã‚’å¾¹åº•çš„ã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    - HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ‡ã‚³ãƒ¼ãƒ‰
    - Unicodeã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆu0026 â†’ &ï¼‰
    - ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‰Šé™¤
    - URLã®æ­£è¦åŒ–ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    """
    try:
        import html as html_module
        import re
        
        # 1. HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆ&amp; â†’ &ï¼‰
        url = html_module.unescape(url)
        
        # 2. URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆ%26 â†’ &ï¼‰
        url = urllib.parse.unquote(url)
        
        # 3. Unicodeã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ï¼ˆTCIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå•é¡Œã®åŸå› ï¼‰
        unicode_escapes = {
            'u0026': '&', '/u0026': '&',
            'u003d': '=', '/u003d': '=',
            'u003f': '?', '/u003f': '?',
            'u0023': '#', '/u0023': '#',
            'u002f': '/', '/u002f': '/',
            'u003a': ':', '/u003a': ':',
            'u002b': '+', '/u002b': '+',
        }
        for escape, char in unicode_escapes.items():
            url = url.replace(escape, char)
        
        # 4. Googleãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        tracking_params = ['&ved=', '?ved=', '&hl=', '?hl=', '&sl=', '&tl=', '&client=', '&prev=', '&sa=', '&source=', '&usg=']
        for param in tracking_params:
            if param in url:
                url = url.split(param)[0]
        
        # 5. æœ«å°¾ã®è¨˜å·ã‚’å‰Šé™¤
        url = url.rstrip('.,;:)"\'')  
        
        # 6. URLã®æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€ï¼ˆæ­£è¦åŒ–ï¼‰
        if url.endswith('/'):
            url = url.rstrip('/')
        
        # 7. ä¸æ­£ãªåˆ¶å¾¡æ–‡å­—ã‚’å‰Šé™¤
        url = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', url)
        
        # 8. URLãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆåŸºæœ¬çš„ãªå½¢å¼ãƒã‚§ãƒƒã‚¯ï¼‰
        if not url.startswith(('http://', 'https://')):
            return None
        
        return url
    except Exception as e:
        return url

def fetch_page_with_browser(url, logger):
    """Browser APIçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ”¹å–„ç‰ˆï¼‰"""
    clean_url_str = clean_url(url)
    if not clean_url_str:
        logger.log(f"  âŒ URLã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—", "ERROR")
        return None, None
    
    logger.log(f"  ğŸŒ Browser APIçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—", "DEBUG")
    if url != clean_url_str:
        logger.log(f"    å…ƒURL: {url[:80]}...", "DEBUG")
        logger.log(f"    ã‚¯ãƒªãƒ¼ãƒ³URL: {clean_url_str[:80]}...", "DEBUG")
    else:
        logger.log(f"    URL: {clean_url_str[:80]}...", "DEBUG")
    
    # è¤‡æ•°æˆ¦ç•¥ã§ãƒªãƒˆãƒ©ã‚¤
    strategies = [
        ('networkidle', 45000),
        ('load', 60000),
        ('domcontentloaded', 30000)
    ]
    
    for wait_type, timeout_ms in strategies:
        try:
            with sync_playwright() as p:
                browser = p.chromium.connect_over_cdp(BROWSER_API_CONFIG['ws_endpoint'])
                page = browser.contexts[0].new_page()
                page.goto(clean_url_str, timeout=timeout_ms, wait_until=wait_type)
                
                # JavaScriptå‹•çš„ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã®å¾…æ©Ÿï¼ˆä¾¡æ ¼è¡¨ç¤ºç”¨ï¼‰
                time.sleep(3)  # åŸºæœ¬å¾…æ©Ÿã‚’3ç§’ã«å»¶é•·
                
                # ä¾¡æ ¼è¦ç´ ã®æ˜ç¤ºçš„ãªå¾…æ©Ÿï¼ˆæœ€å¤§5ç§’ï¼‰
                try:
                    # ä¾¡æ ¼ã‚’å«ã‚€è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
                    page.wait_for_selector('span:has-text("Â¥"), span:has-text("å††"), span:has-text("$"), [class*="price"], [class*="Price"]', timeout=5000, state='visible')
                    logger.log(f"  ğŸ’° ä¾¡æ ¼è¦ç´ ã‚’æ¤œå‡º", "DEBUG")
                except:
                    logger.log(f"  âš ï¸ ä¾¡æ ¼è¦ç´ ã®æ˜ç¤ºçš„ãªå¾…æ©Ÿã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆHTMLå–å¾—ã¯ç¶™ç¶šï¼‰", "DEBUG")
                
                # è¿½åŠ ã®å®‰å…¨å¾…æ©Ÿ
                time.sleep(2)
                
                html_content = page.content()
                page.close()
                browser.close()
                
                if len(html_content) >= 1000:
                    logger.log(f"  âœ… ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ [{wait_type}] ({len(html_content)} chars)", "INFO")
                    return html_content, clean_url_str  # ã‚¯ãƒªãƒ¼ãƒ³URLã‚’è¿”ã™
        except Exception as e:
            if 'Timeout' in str(e):
                logger.log(f"  âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ[{wait_type}]ã€æ¬¡æˆ¦ç•¥è©¦è¡Œ", "DEBUG")
                continue
            logger.log(f"  âŒ ã‚¨ãƒ©ãƒ¼[{wait_type}]: {str(e)[:100]}", "ERROR")
            break
    
    logger.log(f"  âŒ å…¨æˆ¦ç•¥å¤±æ•—", "ERROR")
    return None, None


def search_with_strategy(product_name, site_info, serp_config, logger):
    """æ¤œç´¢æˆ¦ç•¥ï¼ˆSERP APIä½¿ç”¨ï¼‰"""
    site_name = site_info["name"]
    domain = site_info["domain"]
    
    logger.log(f"ğŸ” {site_name} ({domain})ã‚’æ¤œç´¢ä¸­", "INFO")
    
    if not serp_config['available']:
        logger.log(f"  âŒ SERP APIæœªè¨­å®š", "ERROR")
        return []
    
    search_queries = [
        f"{product_name} site:{domain}",
        f"{product_name} price site:{domain}",
        f"{product_name} ä¾¡æ ¼ site:{domain}",
    ]
    
    all_results = []
    
    for query_idx, query in enumerate(search_queries):
        logger.log(f"  ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª{query_idx+1}/3: {query}", "DEBUG")
        
        html = search_google_with_serp(query, serp_config, logger)
        
        if not html:
            time.sleep(1)
            continue
        
        urls = extract_urls_from_html(html, domain, logger)
        
        if urls:
            for url_data in urls[:5]:
                all_results.append({
                    'url': url_data['url'],
                    'site': site_name,
                    'score': url_data.get('score', 0)
                })
            
            logger.log(f"  âœ… {len(urls)}ä»¶ã®URLå–å¾—æˆåŠŸ", "INFO")
            break
        
        time.sleep(1)
    
    if all_results:
        logger.log(f"âœ… {site_name}: {len(all_results)}ä»¶ã®URLå–å¾—", "INFO")
    else:
        logger.log(f"âŒ {site_name}: URLæœªç™ºè¦‹", "ERROR")
    
    return all_results

def calculate_product_name_similarity(name1, name2):
    """è£½å“åã®é¡ä¼¼åº¦ã‚’ç°¡æ˜“è¨ˆç®—ï¼ˆ0.0ã€œ1.0ï¼‰"""
    if not name1 or not name2:
        return 0.0
    
    # æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ï¼‰
    name1_norm = name1.lower().replace(' ', '').replace('-', '')
    name2_norm = name2.lower().replace(' ', '').replace('-', '')
    
    # å®Œå…¨ä¸€è‡´
    if name1_norm == name2_norm:
        return 1.0
    
    # ç‰‡æ–¹ãŒä»–æ–¹ã‚’å«ã‚€
    if name1_norm in name2_norm or name2_norm in name1_norm:
        return 0.8
    
    # å…±é€šæ–‡å­—æ•°ã®å‰²åˆ
    common_chars = set(name1_norm) & set(name2_norm)
    max_len = max(len(name1_norm), len(name2_norm))
    if max_len > 0:
        return len(common_chars) / max_len
    
    return 0.0

def extract_product_info_from_page(html_content, product_name, url, site_name, model, logger):
    """ãƒšãƒ¼ã‚¸HTMLã‹ã‚‰è£½å“æƒ…å ±ã‚’æŠ½å‡º"""
    logger.log(f"  ğŸ¤– Gemini AIã§è£½å“æƒ…å ±ã‚’æŠ½å‡ºä¸­...", "DEBUG")
    
    try:
        # HTMLã®ä¾¡æ ¼é–¢é€£éƒ¨åˆ†ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
        if len(html_content) > 150000:
            logger.log(f"  ğŸ” HTMLè§£æ: {len(html_content)} chars ã‹ã‚‰ä¾¡æ ¼æƒ…å ±ã‚’æ¤œç´¢", "DEBUG")
            
            # ä¾¡æ ¼é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§åˆ†å‰²ã—ã¦é‡è¦éƒ¨åˆ†ã‚’æŠ½å‡º
            price_keywords = ['ä¾¡æ ¼', 'å††', 'Â¥', 'price', 'yen', 'ç¨è¾¼', 'ç¨æŠœ', 'è²©å£²ä¾¡æ ¼', 'å˜ä¾¡', 'mg', 'g', 'mL', 'L', 'USD', '$', 'â‚¬']
            important_chunks = []
            
            # HTMLã‚’è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunk_size = 5000
            for i in range(0, len(html_content), chunk_size):
                chunk = html_content[i:i+chunk_size]
                # ä¾¡æ ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯ã‚’å„ªå…ˆ
                if any(keyword in chunk for keyword in price_keywords):
                    important_chunks.append(chunk)
            
            # é‡è¦ãªãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆï¼ˆæœ€å¤§150K charsï¼‰
            if important_chunks:
                html_content = '\n'.join(important_chunks[:30])  # æœ€å¤§30ãƒãƒ£ãƒ³ã‚¯
                logger.log(f"  âœ‚ï¸ ä¾¡æ ¼é–¢é€£éƒ¨åˆ†ã‚’æŠ½å‡º: {len(html_content)} chars", "DEBUG")
            else:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å‰åŠã‚’ä½¿ç”¨
                html_content = html_content[:150000]
                logger.log(f"  âœ‚ï¸ HTMLåˆ‡ã‚Šè©°ã‚ï¼ˆå‰åŠï¼‰: 150000 chars", "DEBUG")
        else:
            logger.log(f"  ğŸ“„ HTMLå…¨ä½“ã‚’ä½¿ç”¨: {len(html_content)} chars", "DEBUG")
        
        prompt = f"""
ã‚ãªãŸã¯åŒ–å­¦è©¦è–¬ã®Webã‚µã‚¤ãƒˆã‹ã‚‰ã®è£½å“æƒ…å ±æŠ½å‡ºã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®HTMLã‹ã‚‰ã€è£½å“ã®è©³ç´°æƒ…å ±ã¨**ç‰¹ã«ä¾¡æ ¼æƒ…å ±**ã‚’å¾¹åº•çš„ã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ã€‘ä¾¡æ ¼æƒ…å ±ã®æ¤œç´¢æ‰‹é †:
1. ã¾ãšã€ä»¥ä¸‹ã®HTMLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã—ã¦ãã ã•ã„:
   - <td>ã‚„<span>ã‚¿ã‚°å†…ã®ã€ŒÂ¥ã€ã€Œå††ã€ã‚’å«ã‚€ãƒ†ã‚­ã‚¹ãƒˆ
   - class="price"ã€class="product-price"ç­‰ã®ä¾¡æ ¼é–¢é€£ã‚¯ãƒ©ã‚¹
   - JavaScriptã®å¤‰æ•°å®šç¾©ï¼ˆprice:ã€yen:ç­‰ï¼‰
   - ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ å†…ã®ä¾¡æ ¼åˆ—
   - ã€Œç¨è¾¼ã€ã€Œç¨æŠœã€ã€Œè²©å£²ä¾¡æ ¼ã€ã€Œå˜ä¾¡ã€ç­‰ã®ãƒ©ãƒ™ãƒ«ã®è¿‘ã

2. å®¹é‡ãƒ»ã‚µã‚¤ã‚ºæƒ…å ±ã‚‚åŒæ™‚ã«æŠ½å‡º:
   - ã€Œ1mgã€ã€Œ5mgã€ã€Œ10mgã€ã€Œ100mgã€ã€Œ1gã€ã€Œ5gã€ç­‰
   - ã€Œ1mLã€ã€Œ10mLã€ã€Œ100mLã€ã€Œ1Lã€ç­‰
   - ã‚µã‚¤ã‚ºã¨ä¾¡æ ¼ã¯é€šå¸¸ã€åŒã˜è¡Œã‚„è¿‘æ¥ã—ãŸè¦ç´ ã«ã‚ã‚Šã¾ã™

3. è¤‡æ•°ã®ä¾¡æ ¼ãŒã‚ã‚‹å ´åˆ:
   - **å…¨ã¦ã®ä¾¡æ ¼ã¨ã‚µã‚¤ã‚ºã®çµ„ã¿åˆã‚ã›ã‚’æŠ½å‡º**ã—ã¦ãã ã•ã„
   - è¦‹ã¤ã‹ã£ãŸä¾¡æ ¼ã¯1ã¤ã‚‚æ¼ã‚‰ã•ãšå…¨ã¦è¨˜éŒ²ã—ã¦ãã ã•ã„

ã€æŠ½å‡ºã™ã‚‹æƒ…å ±ã€‘
- productName: è£½å“åï¼ˆåŒ–åˆç‰©åï¼‰
- modelNumber: ã‚«ã‚¿ãƒ­ã‚°ç•ªå·ã¾ãŸã¯CASç•ªå·
- manufacturer: è£½é€ å…ƒã¾ãŸã¯ãƒ–ãƒ©ãƒ³ãƒ‰å
- offers: ä¾¡æ ¼æƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆ**é‡è¦**: è¦‹ã¤ã‹ã£ãŸä¾¡æ ¼ã¯å…¨ã¦å«ã‚ã‚‹ï¼‰

ã€offersé…åˆ—ã®å„è¦ç´ ã€‘
- size: å®¹é‡ãƒ»ã‚µã‚¤ã‚ºï¼ˆä¾‹: "1mg", "5mg", "10mg", "100g"ç­‰ï¼‰
- price: ä¾¡æ ¼ï¼ˆæ•°å€¤ã®ã¿ã€ã‚«ãƒ³ãƒãªã—ï¼‰
- inStock: åœ¨åº«çŠ¶æ³ï¼ˆçœŸå½å€¤: true/falseã€ä¸æ˜ãªå ´åˆã¯trueï¼‰

ã€ä¾¡æ ¼ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ä¾‹ã€‘ï¼ˆã“ã‚Œã‚‰ã‚’å…¨ã¦èªè­˜ã—ã¦ãã ã•ã„ï¼‰:
- æ—¥æœ¬èª: "Â¥34,000", "34,000å††", "ç¨è¾¼Â¥32,000", "ç¨æŠœ Â¥30,000"
- è‹±èª: "$340.00", "USD 340", "â‚¬300"
- ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼: "1mg | Â¥14,800", "5mg | Â¥36,100"
- ãƒªã‚¹ãƒˆå½¢å¼: "â€¢ 1mg: 14,800å††"

ã€ä¾¡æ ¼æŠ½å‡ºã®å¤‰æ›è¦å‰‡ã€‘
- "Â¥34,000" â†’ 34000
- "34,000å††" â†’ 34000  
- "$340.00" â†’ 340
- "ç¨æŠœ Â¥32,000" â†’ 32000
- ã‚«ãƒ³ãƒã€é€šè²¨è¨˜å·ã¯å…¨ã¦å‰Šé™¤ã—ã€æ•°å€¤ã®ã¿ã«ã™ã‚‹

ã€å‡ºåŠ›å½¢å¼ã€‘å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›:
{{
  "productName": "Y-27632 dihydrochloride",
  "modelNumber": "146986-50-7",
  "manufacturer": "Sigma-Aldrich",
  "offers": [
    {{"size": "1mg", "price": 34000, "inStock": true}},
    {{"size": "5mg", "price": 54000, "inStock": true}},
    {{"size": "10mg", "price": 78000, "inStock": true}}
  ]
}}

**æ³¨æ„**: ä¾¡æ ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã¿ offers ã‚’ç©ºé…åˆ— [] ã«ã—ã¦ãã ã•ã„ã€‚
HTMLã«ä¾¡æ ¼æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€å¿…ãšæŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
{html_content}

ã€ã‚½ãƒ¼ã‚¹URLã€‘
{url}

å¿…ãšJSONå½¢å¼ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚
"""
        
        # ãƒ‡ãƒãƒƒã‚°: HTMLã«ä¾¡æ ¼æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        price_indicators = [('Â¥', 'yen_symbol'), ('å††', 'yen_kanji'), ('price', 'price_en'), 
                           ('ä¾¡æ ¼', 'price_ja'), ('ç¨è¾¼', 'tax_included'), ('ç¨æŠœ', 'tax_excluded')]
        found_indicators = []
        for indicator, name in price_indicators:
            count = html_content.count(indicator)
            if count > 0:
                found_indicators.append(f"{name}:{count}")
        
        if found_indicators:
            logger.log(f"  ğŸ” HTMLå†…ä¾¡æ ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {', '.join(found_indicators)}", "DEBUG")
        else:
            logger.log(f"  âš ï¸ HTMLå†…ã«ä¾¡æ ¼é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "WARNING")
        
        # Gemini APIå‘¼ã³å‡ºã—ï¼ˆè¤‡æ•°å›è©¦è¡Œï¼‰
        max_retries = 2
        best_response = None
        best_response_text = ""
        
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    logger.log(f"  ğŸ”„ å†è©¦è¡Œ {attempt+1}/{max_retries}...", "DEBUG")
                
                # è©¦è¡Œå›æ•°ã«å¿œã˜ã¦generation_configã‚’èª¿æ•´
                generation_config = {
                    "temperature": 0.1 + (attempt * 0.2),  # 0.1 -> 0.3
                    "top_p": 0.95,
                    "top_k": 40
                }
                
                response = model.generate_content(prompt, generation_config=generation_config)
                response_text = response.text.strip()
                
                logger.log(f"  ğŸ“¨ Gemini APIå¿œç­”å—ä¿¡ [{attempt+1}] ({len(response_text)} chars)", "DEBUG")
                
                # æœ‰åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆoffersãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼‰
                if len(response_text) > 200 and '"offers"' in response_text:
                    # ä¾¡æ ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„
                    best_response_text = response_text
                    logger.log(f"  âœ… æœ‰åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—", "DEBUG")
                    break
                elif len(response_text) > len(best_response_text):
                    # ã‚ˆã‚Šé•·ã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿æŒ
                    best_response_text = response_text
            except Exception as e:
                logger.log(f"  âš ï¸ è©¦è¡Œ{attempt+1}å¤±æ•—: {str(e)}", "WARNING")
                continue
        
        response_text = best_response_text
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç•°å¸¸ã«çŸ­ã„å ´åˆã¯è©³ç´°ã‚’è¡¨ç¤º
        if len(response_text) < 200:
            logger.log(f"  âš ï¸ Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒçŸ­ã„: {response_text}", "WARNING")
            # HTMLã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰
            html_sample = html_content[:500].replace('\n', ' ')[:200]
            logger.log(f"  ğŸ“„ HTMLã‚µãƒ³ãƒ—ãƒ«: {html_sample}...", "DEBUG")
        
        # JSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        response_text = re.sub(r'^```json\s*', '', response_text)
        response_text = re.sub(r'^```\s*', '', response_text)
        response_text = re.sub(r'\s*```$', '', response_text)
        response_text = response_text.strip()
        
        # JSONãƒ‘ãƒ¼ã‚¹
        product_info = json.loads(response_text)
        
        # è£½å“åã®é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
        extracted_name = product_info.get('productName', '')
        similarity = calculate_product_name_similarity(product_name, extracted_name)
        logger.log(f"  ğŸ” è£½å“åé¡ä¼¼åº¦: {similarity:.2f} (æ¤œç´¢: {product_name} vs æŠ½å‡º: {extracted_name})", "DEBUG")
        
        if similarity < 0.3:
            logger.log(f"  âš ï¸ è£½å“åã®é¡ä¼¼åº¦ãŒä½ã„ï¼ˆ{similarity:.2f}ï¼‰ã€‚åˆ¥ã®è£½å“ã®å¯èƒ½æ€§ã‚ã‚Šã€‚", "WARNING")
        
        # ãƒ‡ãƒ¼ã‚¿å‹æ¤œè¨¼
        if 'offers' in product_info and isinstance(product_info['offers'], list):
            valid_offers = []
            for offer in product_info['offers']:
                if 'price' in offer:
                    try:
                        if isinstance(offer['price'], str):
                            price_str = offer['price'].replace(',', '').replace('Â¥', '').replace('å††', '').replace('$', '').replace('â‚¬', '').strip()
                            offer['price'] = float(price_str)
                        else:
                            offer['price'] = float(offer['price'])
                        
                        if offer['price'] > 0:
                            valid_offers.append(offer)
                    except:
                        pass
            
            product_info['offers'] = valid_offers
        
        if product_info.get('offers'):
            logger.log(f"  âœ… {len(product_info['offers'])}ä»¶ã®ä¾¡æ ¼æƒ…å ±ã‚’æŠ½å‡º", "INFO")
            for i, offer in enumerate(product_info['offers'][:3]):
                logger.log(f"    - {offer.get('size', 'N/A')}: Â¥{int(offer.get('price', 0)):,}", "DEBUG")
        else:
            logger.log(f"  âš ï¸ ä¾¡æ ¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
            if found_indicators:
                logger.log(f"  ğŸ’¡ ãƒ’ãƒ³ãƒˆ: HTMLå†…ã«ä¾¡æ ¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯å­˜åœ¨ã—ã¾ã™ãŒã€GeminiãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ", "WARNING")
                
                # ãƒ‡ãƒãƒƒã‚°: HTMLã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                try:
                    import os
                    debug_dir = '/mnt/user-data/outputs/html_debug'
                    os.makedirs(debug_dir, exist_ok=True)
                    debug_file = f"{debug_dir}/{site_name.replace('/', '_').replace(' ', '_')}_sample.html"
                    with open(debug_file, 'w', encoding='utf-8') as f:
                        f.write(f"<!-- URL: {url} -->\n")
                        f.write(f"<!-- Found indicators: {', '.join(found_indicators)} -->\n")
                        f.write(html_content[:50000])  # æœ€åˆã®50KBã‚’ä¿å­˜
                    logger.log(f"  ğŸ’¾ ãƒ‡ãƒãƒƒã‚°ç”¨HTMLä¿å­˜: {os.path.basename(debug_file)}", "DEBUG")
                except Exception as e:
                    logger.log(f"  âš ï¸ HTMLä¿å­˜å¤±æ•—: {e}", "DEBUG")
        
        return product_info
        
    except json.JSONDecodeError as e:
        logger.log(f"  âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        logger.log(f"  ğŸ“„ ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text[:500]}", "DEBUG")
        return None
    except Exception as e:
        logger.log(f"  âŒ è£½å“æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}", "ERROR")
        import traceback
        logger.log(f"  ğŸ“‹ è©³ç´°: {traceback.format_exc()}", "DEBUG")
        return None

def main():
    st.markdown('<h1 class="main-header">ğŸ§ª åŒ–å­¦è©¦è–¬ ä¾¡æ ¼æ¯”è¼ƒã‚·ã‚¹ãƒ†ãƒ ï¼ˆBrowser APIç‰ˆ v3.3ï¼‰</h1>', unsafe_allow_html=True)
    
    serp_config = check_serp_api_config()
    
    if serp_config['available'] and BROWSER_API_CONFIG['available']:
        st.markdown(
            f'<div class="api-status api-success">âœ… LLM: Gemini 2.5 Pro | SERP API: {serp_config["zone_name"]} | Browser API: scraping_browser1</div>',
            unsafe_allow_html=True
        )
    else:
        st.markdown(
            '<div class="api-status api-warning">âš ï¸ APIæœªè¨­å®š</div>',
            unsafe_allow_html=True
        )
        return
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        product_name = st.text_input(
            "ğŸ” è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            value="Y-27632",
            placeholder="ä¾‹: Y-27632, DMSO, Trizol, Quinpirole"
        )
    
    with col2:
        max_sites = st.number_input(
            "æœ€å¤§æ¤œç´¢ã‚µã‚¤ãƒˆæ•°",
            min_value=1,
            max_value=11,
            value=11,
            step=1
        )
    
    st.markdown("---")
    
    if st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True):
        if not product_name:
            st.warning("âš ï¸ è£½å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        st.markdown("### ğŸ“ å‡¦ç†ãƒ­ã‚°")
        log_container = st.empty()
        logger = RealTimeLogger(log_container)
        
        start_time = time.time()
        logger.log(f"ğŸš€ å‡¦ç†é–‹å§‹: {product_name}", "INFO")
        logger.log(f"ğŸ¤– LLM: Gemini 2.5 Pro", "INFO")
        logger.log(f"ğŸ” Googleæ¤œç´¢: SERP API (Zone: {serp_config['zone_name']})", "INFO")
        logger.log(f"ğŸŒ ãƒšãƒ¼ã‚¸å–å¾—: Browser API (Zone: scraping_browser1)", "INFO")
        logger.log(f"ğŸ¯ å¯¾è±¡ã‚µã‚¤ãƒˆæ•°: {max_sites}ã‚µã‚¤ãƒˆ", "INFO")
        
        model = setup_gemini()
        if not model:
            st.error("âŒ Gemini APIã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        all_products = []
        sites_to_search = dict(list(TARGET_SITES.items())[:max_sites])
        
        for site_idx, (site_key, site_info) in enumerate(sites_to_search.items(), 1):
            logger.log(f"\n--- ã‚µã‚¤ãƒˆ {site_idx}/{max_sites} ---", "INFO")
            
            search_results = search_with_strategy(product_name, site_info, serp_config, logger)
            
            if not search_results:
                logger.log(f"â­ï¸  æ¬¡ã®ã‚µã‚¤ãƒˆã¸", "DEBUG")
                time.sleep(2)
                continue
            
            # æœ€ã‚‚ã‚¹ã‚³ã‚¢ãŒé«˜ã„URLã‚’ä½¿ç”¨
            search_results.sort(key=lambda x: x.get('score', 0), reverse=True)
            result = search_results[0]
            
            logger.log(f"ğŸ¯ ãƒˆãƒƒãƒ—URL: {result['url'][:80]}...", "INFO")
            
            # Browser APIçµŒç”±ã§ãƒšãƒ¼ã‚¸å–å¾—ï¼ˆã‚¯ãƒªãƒ¼ãƒ³URLã‚’å–å¾—ï¼‰
            html_content, clean_url = fetch_page_with_browser(result['url'], logger)
            
            if html_content and clean_url:
                page_info = extract_product_info_from_page(
                    html_content, 
                    product_name, 
                    clean_url,  # ã‚¯ãƒªãƒ¼ãƒ³URLã‚’ä½¿ç”¨
                    result.get('site', 'unknown'),
                    model, 
                    logger
                )
                
                if page_info:
                    page_info['source_site'] = result['site']
                    page_info['source_url'] = clean_url  # ã‚¯ãƒªãƒ¼ãƒ³URLã‚’ä¿å­˜
                    all_products.append(page_info)
                    logger.log(f"âœ… {result['site']}: è£½å“æƒ…å ±å–å¾—æˆåŠŸ", "INFO")
                else:
                    logger.log(f"âš ï¸ {result['site']}: AIè§£æå¤±æ•—", "WARNING")
            else:
                logger.log(f"âŒ {result['site']}: ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—", "ERROR")
            
            time.sleep(2)
        
        elapsed_time = time.time() - start_time
        logger.log(f"\nğŸ‰ å‡¦ç†å®Œäº†: {elapsed_time:.1f}ç§’", "INFO")
        logger.log(f"ğŸ“Š å–å¾—æˆåŠŸ: {len(all_products)}/{max_sites}ã‚µã‚¤ãƒˆ", "INFO")
        
        st.markdown("---")
        st.markdown("## ğŸ“‹ æ¤œç´¢çµæœ")
        
        if not all_products:
            st.error("âŒ è£½å“æƒ…å ±ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            st.info("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: è£½å“åã‚’å¤‰æ›´ã™ã‚‹ã‹ã€æ¤œç´¢å¯¾è±¡ã‚µã‚¤ãƒˆã‚’èª¿æ•´ã—ã¦ãã ã•ã„")
            return
        
        with_price = [p for p in all_products if p.get('offers')]
        without_price = [p for p in all_products if not p.get('offers')]
        
        st.success(f"âœ… {len(all_products)}ä»¶ã®è£½å“æƒ…å ±ã‚’å–å¾—ï¼ˆä¾¡æ ¼æƒ…å ±ã‚ã‚Š: {len(with_price)}ä»¶ã€å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’ï¼‰")
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤º
        table_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A') or '',
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A'),
                'ãƒªãƒ³ã‚¯å…ˆ': product.get('source_url', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['å®¹é‡'] = offer.get('size', 'N/A')
                    
                    try:
                        price = offer.get('price', 0)
                        if isinstance(price, (int, float)) and price > 0:
                            row['ä¾¡æ ¼'] = f"Â¥{int(price):,}"
                        else:
                            row['ä¾¡æ ¼'] = 'N/A'
                    except:
                        row['ä¾¡æ ¼'] = 'N/A'
                    
                    row['åœ¨åº«æœ‰ç„¡'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    table_data.append(row)
            else:
                row = base_info.copy()
                row['å®¹é‡'] = 'N/A'
                row['ä¾¡æ ¼'] = 'N/A'
                row['åœ¨åº«æœ‰ç„¡'] = 'N/A'
                table_data.append(row)
        
        if table_data:
            df_display = pd.DataFrame(table_data)
            # åˆ—ã®é †åºã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
            column_order = ['è£½å“å', 'è²©å£²å…ƒ', 'å‹ç•ª', 'ãƒ¡ãƒ¼ã‚«ãƒ¼', 'ãƒªãƒ³ã‚¯å…ˆ', 'å®¹é‡', 'ä¾¡æ ¼', 'åœ¨åº«æœ‰ç„¡']
            # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é¸æŠ
            existing_columns = [col for col in column_order if col in df_display.columns]
            df_display = df_display[existing_columns]
            
            # ãƒ‡ãƒãƒƒã‚°: ãƒªãƒ³ã‚¯å…ˆåˆ—ã®å€¤ã‚’ç¢ºèª
            if 'ãƒªãƒ³ã‚¯å…ˆ' in df_display.columns:
                logger.log(f"  ğŸ”— ãƒªãƒ³ã‚¯å…ˆåˆ—ã‚’ç¢ºèª: {df_display['ãƒªãƒ³ã‚¯å…ˆ'].head(3).tolist()}", "DEBUG")
            else:
                logger.log(f"  âš ï¸ ãƒªãƒ³ã‚¯å…ˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "WARNING")
            
            st.dataframe(df_display, use_container_width=True, height=600)
        
        # CSVå‡ºåŠ›
        st.markdown("---")
        st.markdown("## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        export_data = []
        for product in all_products:
            base_info = {
                'è£½å“å': product.get('productName', 'N/A'),
                'è²©å£²å…ƒ': product.get('source_site', 'N/A'),
                'å‹ç•ª': product.get('modelNumber', 'N/A') or '',
                'ãƒ¡ãƒ¼ã‚«ãƒ¼': product.get('manufacturer', 'N/A'),
                'ãƒªãƒ³ã‚¯å…ˆ': product.get('source_url', 'N/A')
            }
            
            if 'offers' in product and product['offers']:
                for offer in product['offers']:
                    row = base_info.copy()
                    row['å®¹é‡'] = offer.get('size', 'N/A')
                    
                    try:
                        price = offer.get('price', 0)
                        if isinstance(price, (int, float)) and price > 0:
                            row['ä¾¡æ ¼'] = f"Â¥{int(price):,}"
                        else:
                            row['ä¾¡æ ¼'] = 'N/A'
                    except:
                        row['ä¾¡æ ¼'] = 'N/A'
                    
                    row['åœ¨åº«æœ‰ç„¡'] = 'æœ‰' if offer.get('inStock') else 'ç„¡'
                    export_data.append(row)
            else:
                row = base_info.copy()
                row['å®¹é‡'] = 'N/A'
                row['ä¾¡æ ¼'] = 'N/A'
                row['åœ¨åº«æœ‰ç„¡'] = 'N/A'
                export_data.append(row)
        
        df = pd.DataFrame(export_data)
        
        # CSVå‡ºåŠ›ã®åˆ—é †åºã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        csv_column_order = ['è£½å“å', 'è²©å£²å…ƒ', 'å‹ç•ª', 'ãƒ¡ãƒ¼ã‚«ãƒ¼', 'ãƒªãƒ³ã‚¯å…ˆ', 'å®¹é‡', 'ä¾¡æ ¼', 'åœ¨åº«æœ‰ç„¡']
        existing_csv_columns = [col for col in csv_column_order if col in df.columns]
        df = df[existing_csv_columns]
        
        csv_buffer = StringIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
        csv_data = csv_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=f"chemical_prices_{product_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )

if __name__ == "__main__":
    main()
